{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to agate-examples\n\n\n[A very young work in progress]\n\n\nThe goal of this site is to have a store of examples using the \nagate\n Python library. While the \nCookbook\n and \nTutorial\n have some good examples, these are variations that I would like to remember. Perhaps others can benefit from this as well.\n\n\nA start\n\n\n\n\nimport\n\n\nfilters\n\n\ncompute", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-agate-examples", 
            "text": "[A very young work in progress]  The goal of this site is to have a store of examples using the  agate  Python library. While the  Cookbook  and  Tutorial  have some good examples, these are variations that I would like to remember. Perhaps others can benefit from this as well.", 
            "title": "Welcome to agate-examples"
        }, 
        {
            "location": "/#a-start", 
            "text": "import  filters  compute", 
            "title": "A start"
        }, 
        {
            "location": "/imports/", 
            "text": "Imports \n Exports\n\n\nTo do:\n\n\n\n\nCreate a data example that can be used throughout. Or a small set of them.\n\n\nadd zfill example from schools, or link to it if elsewhere\n\n\nadd from xlsx hints if any \n\n\nadd a date example for setting column types.\n\n\nfigure out a loop of imports for a series of files that are similar except for an id or filename. So I can update the list of IDs or names and they would all import, then I can \n.merge()\n them together.\n\n\n\n\nFrom csv\n\n\nI'm usually pulling from .csv files and it's pretty normal.::\n\n\n  table = agate.Table.from_csv('filename.csv')\n\n\n\n\nSet column type\n\n\nBut more often than not I have to explicitly set a certain column::\n\n\nspecified_types = {\n      'column_name_one': agate.Text(),\n      'column_name_two': agate.Number()\n  }\n\n\ntable = agate.Table.from_csv('filename.csv', column_types=specified_types)\n\n\nI need a date example here.\n\n\nFill in zero-padded fields\n\n\nThis is covered in the \ncompute section\n.\n\n\nAdd timezone to a date\n\n\nI had a case where my original data was in UTC time, but I needed to convert it to Central time. According to \nagate docs on dates\n, it imports naive of timezone, so you have to set it to a specific one before you can convert it to another:\n\n\n  import pytz\n\n  ## sets date coming in at UTC so we can convert it later\n  specified_type = {\n      'dateTime': agate.DateTime(timezone=pytz.utc)\n  }\n\n  # import all the file.\n  flow_08154700 = agate.Table.from_json('../downloads/flow-08154700.json', column_types=specified_type, key='records')\n\n\n\n\nNote you need to import the \npytz library\n. You can get a \nlist timezones here\n (or make your own).\n\n\nIn this case, the column name for the date in my data was \"dateTime\", not to be confused with the \n.DateTime\n data type used in the definition.\n\n\nSee the \ncompute page\n for the conversion.\n\n\nImport from json\n\n\nThe example above also shows importing from a \n.json\n file::\n\n\n    flow_08154700 = agate.Table.from_json('../downloads/flow-08154700.json', column_types=specified_type, key='records')\n\n\n\n\nNote you have to have a key, which is the part of the json file that defines the list of records (notably called \"records\" in that instance. \n\n\nMy data looked like this::\n\n\n{\n    \"siteId\":\"08154700\",\n    \"siteNumber\":\"08154700\",\n    \"siteName\":\"Bull Ck at Loop 360 nr Austin, TX\",\n    \"bankFullStage\":5,\n    \"value1Type\":\"Stage\",\n    \"value2Type\":\"Flow\",\n    \"records\":[\n        {\n            \"dateTime\":\"2017-08-27T03:00:00Z\",\n            \"value1\":4.46,\n            \"value1Qualifier\":\"P\",\n            \"value2\":414.00,\n            \"value2Qualifier\":\"P\"\n        },\n        {\n            \"dateTime\":\"2017-08-27T02:55:00Z\",\n            \"value1\":4.38,\n            \"value1Qualifier\":\"P\",\n            \"value2\":383.00,\n            \"value2Qualifier\":\"P\"\n    }\n]}\n\n\n\nBut it was only the \"records\" nodes that I needed, so the value I needed for the key was \nrecords\n. I'm not even sure how I would get the other information at the top.", 
            "title": "Imports"
        }, 
        {
            "location": "/imports/#imports-exports", 
            "text": "To do:   Create a data example that can be used throughout. Or a small set of them.  add zfill example from schools, or link to it if elsewhere  add from xlsx hints if any   add a date example for setting column types.  figure out a loop of imports for a series of files that are similar except for an id or filename. So I can update the list of IDs or names and they would all import, then I can  .merge()  them together.", 
            "title": "Imports &amp; Exports"
        }, 
        {
            "location": "/imports/#from-csv", 
            "text": "I'm usually pulling from .csv files and it's pretty normal.::    table = agate.Table.from_csv('filename.csv')", 
            "title": "From csv"
        }, 
        {
            "location": "/imports/#set-column-type", 
            "text": "But more often than not I have to explicitly set a certain column::  specified_types = {\n      'column_name_one': agate.Text(),\n      'column_name_two': agate.Number()\n  }  table = agate.Table.from_csv('filename.csv', column_types=specified_types)  I need a date example here.", 
            "title": "Set column type"
        }, 
        {
            "location": "/imports/#fill-in-zero-padded-fields", 
            "text": "This is covered in the  compute section .", 
            "title": "Fill in zero-padded fields"
        }, 
        {
            "location": "/imports/#add-timezone-to-a-date", 
            "text": "I had a case where my original data was in UTC time, but I needed to convert it to Central time. According to  agate docs on dates , it imports naive of timezone, so you have to set it to a specific one before you can convert it to another:    import pytz\n\n  ## sets date coming in at UTC so we can convert it later\n  specified_type = {\n      'dateTime': agate.DateTime(timezone=pytz.utc)\n  }\n\n  # import all the file.\n  flow_08154700 = agate.Table.from_json('../downloads/flow-08154700.json', column_types=specified_type, key='records')  Note you need to import the  pytz library . You can get a  list timezones here  (or make your own).  In this case, the column name for the date in my data was \"dateTime\", not to be confused with the  .DateTime  data type used in the definition.  See the  compute page  for the conversion.", 
            "title": "Add timezone to a date"
        }, 
        {
            "location": "/imports/#import-from-json", 
            "text": "The example above also shows importing from a  .json  file::      flow_08154700 = agate.Table.from_json('../downloads/flow-08154700.json', column_types=specified_type, key='records')  Note you have to have a key, which is the part of the json file that defines the list of records (notably called \"records\" in that instance.   My data looked like this::  {\n    \"siteId\":\"08154700\",\n    \"siteNumber\":\"08154700\",\n    \"siteName\":\"Bull Ck at Loop 360 nr Austin, TX\",\n    \"bankFullStage\":5,\n    \"value1Type\":\"Stage\",\n    \"value2Type\":\"Flow\",\n    \"records\":[\n        {\n            \"dateTime\":\"2017-08-27T03:00:00Z\",\n            \"value1\":4.46,\n            \"value1Qualifier\":\"P\",\n            \"value2\":414.00,\n            \"value2Qualifier\":\"P\"\n        },\n        {\n            \"dateTime\":\"2017-08-27T02:55:00Z\",\n            \"value1\":4.38,\n            \"value1Qualifier\":\"P\",\n            \"value2\":383.00,\n            \"value2Qualifier\":\"P\"\n    }\n]}  But it was only the \"records\" nodes that I needed, so the value I needed for the key was  records . I'm not even sure how I would get the other information at the top.", 
            "title": "Import from json"
        }, 
        {
            "location": "/filters/", 
            "text": "Filters: Using .where()\n\n\nMost of these examples are where I am creating a new table with a subselection of rows from the base table:\n\n\n  newtable = oldtable.where(lamda row: row['Column_Name'] == 'something')\n\n\n\n\nLamda is a function that happens just once in place. In the example above, I'm looking through each row, specifically at the column \nColumn_Name\n for the text 'something'. If it matches, I keep it and put it in the newtable. If not, it is discarded.\n\n\nTo add here:\n\n\n\n\nwhere between dates\n\n\ngrep\n\n\nfigure out how to use \n.values_distinct()\n in such a way it makes a nice list.\n\n\n\n\nWhere boolean field is True\n\n\nChecking if a boolean field is true or false:\n\n\n  charters = campus.where(lambda row: row['CFLCHART'] == 1)\n\n\n\n\n(Question to self: Test of this will work as ``== True```.)\n\n\nWhere boolean field is not True\n\n\nWhen you want fields that don't match your test.:\n\n\ntraditional = campus.where(lambda row: row['CFLCHART'] != 1)\n\n\n\n\nIn this case, we are testing 'CFLCHART' to make sure it does NOT equal 1.\n\n\n(Note to self: Test if this would catch nulls.)\n\n\nBy position in string\n\n\nIf you want to look at a specific position in a string and then check it:\n\n\n  charters = campus.where(lambda row: row['Campus_ID'][3] == '8')\n\n\n\n\nThe example is searching the \nCampus_ID\n field looking at the fourth character, checking to see if it is the text \n'8'\n.\n\n\nCheck for blank cells\n\n\nIn this example, I wanted to exclude rows that were null or blank in the \nCampus\n column:\n\n\n  nodistrict = raw.where(lambda row: row['CAMPUS'] is not None)\n\n\n\n\nFilter by date\n\n\nIn this case, I wanted only rows that had a \nCentralTime\n date on or after August 25. The trick here was importing \ndatetime\n, and setting that reference as below. It wasn't intuitive for me from the docs.:\n\n\n    import datetime\n\n    # filter for Aug. 25th or later\n    flow_harvey = flow_central.where(\n        lambda row: datetime.date(2017, 8, 25) \n= row['CentralTime'].date()\n        )", 
            "title": "Filters"
        }, 
        {
            "location": "/filters/#filters-using-where", 
            "text": "Most of these examples are where I am creating a new table with a subselection of rows from the base table:    newtable = oldtable.where(lamda row: row['Column_Name'] == 'something')  Lamda is a function that happens just once in place. In the example above, I'm looking through each row, specifically at the column  Column_Name  for the text 'something'. If it matches, I keep it and put it in the newtable. If not, it is discarded.  To add here:   where between dates  grep  figure out how to use  .values_distinct()  in such a way it makes a nice list.", 
            "title": "Filters: Using .where()"
        }, 
        {
            "location": "/filters/#where-boolean-field-is-true", 
            "text": "Checking if a boolean field is true or false:    charters = campus.where(lambda row: row['CFLCHART'] == 1)  (Question to self: Test of this will work as ``== True```.)", 
            "title": "Where boolean field is True"
        }, 
        {
            "location": "/filters/#where-boolean-field-is-not-true", 
            "text": "When you want fields that don't match your test.:  traditional = campus.where(lambda row: row['CFLCHART'] != 1)  In this case, we are testing 'CFLCHART' to make sure it does NOT equal 1.  (Note to self: Test if this would catch nulls.)", 
            "title": "Where boolean field is not True"
        }, 
        {
            "location": "/filters/#by-position-in-string", 
            "text": "If you want to look at a specific position in a string and then check it:    charters = campus.where(lambda row: row['Campus_ID'][3] == '8')  The example is searching the  Campus_ID  field looking at the fourth character, checking to see if it is the text  '8' .", 
            "title": "By position in string"
        }, 
        {
            "location": "/filters/#check-for-blank-cells", 
            "text": "In this example, I wanted to exclude rows that were null or blank in the  Campus  column:    nodistrict = raw.where(lambda row: row['CAMPUS'] is not None)", 
            "title": "Check for blank cells"
        }, 
        {
            "location": "/filters/#filter-by-date", 
            "text": "In this case, I wanted only rows that had a  CentralTime  date on or after August 25. The trick here was importing  datetime , and setting that reference as below. It wasn't intuitive for me from the docs.:      import datetime\n\n    # filter for Aug. 25th or later\n    flow_harvey = flow_central.where(\n        lambda row: datetime.date(2017, 8, 25)  = row['CentralTime'].date()\n        )", 
            "title": "Filter by date"
        }, 
        {
            "location": "/compute/", 
            "text": "New columns: Using .compute()\n\n\nCreating new columns based on something in the data.\n\n\n[I need to explain a basic .compute() here.]\n\n\nItems to add:\n\n\n\n\nnew date?\n\n\n\n\nzfill: Fix zero-padded IDs\n\n\nIn school data from TEA, the \nCampus_ID\n column is supposed to be a 9-digit column, and any school that is less than that should have zeros at the beginning. Sometimes .csv files saved from Excel loose this zero padding, but we can replace it:\n\n\n  zfilled = nodistrict.compute([\n      ('New_Campus_ID', agate.Formula(agate.Text(), lambda r: r['CAMPUS'].zfill(9))),\n      ('New_District_ID', agate.Formula(agate.Text(), lambda r: r['DISTRICT'].zfill(6)))\n  ])\n\n\n\n\nFor the first one, we created \nNew_Campus_ID\n by going through the \nCampus\n column and using the Python method \n.zfill()\n to set the number of characters to pad with zeros. For \n['Campus']\n we used \n9\n and for \n['District']\n we use \n6\n.\n\n\nMapped translation\n\n\nWe want to create a new field that inserts the longer explanation based on the a one-letter designation in the data.\n\n\nFirst we have the map:\n\n\n  rating_map = {\n      'I': 'Improvement required',\n      'M': 'Met standard',\n      'A': 'Met alternative standard',\n      'X': 'Not rated',\n      'Z': 'Not rated',\n      '': 'Not rated'\n  }\n\n\n\n\nThen we need a function that we will run the compute through to get the proper rating match:\n\n\n  def map_rating(rating):\n      rating = rating.strip()\n      return rating_map[rating]\n\n\n\n\nThen we create the new table. We are looking at the field \nC_RATING\n for the single-letter values. (This was originally used as a method on an export command, so it should be tested in this configuration):\n\n\n  rated = unrated.compute([\n      ('mapped_rating',\n       agate.Formula(agate.Text(),\n       lambda r: map_rating(r['C_RATING']))\n      )\n  ])\n\n\n\n\nIn then end, we have a new column called \nmapped_rating\n that includes values like \"Met Standard\".\n\n\nConverting timezones\n\n\nI had data that came in UTC time, but I wanted to display it in Central Time. You do first need to make sure your datatime is not naive and has a timezone, perhaps when :ref:\nyou import it \nimporttime\n.\n\n\nIn this case, I had a field \ndateTime\n that was in UTC, that I need to convert to Central Time. This requires  the \npytz library \nhttp://pytz.sourceforge.net/index.html?highlight=list%20timezones#\n_, as well:\n\n\n  ## setting central time\n  central = pytz.timezone('US/Central')\n\n  ## formula to do the converstion from the 'dateTime' field\n  time_shifter = agate.Formula(agate.DateTime(), lambda r: r['dateTime'].astimezone(central))\n\n  ## create column and call the formula above\n  flow_central = flow_data.compute([\n          ('CentralTime', time_shifter),\n      ])\n\n\n\n\nThis gives me the new column \nCentral Time\n.\n\n\nRewrite date/time in another format\n\n\nI had a case where my Tableau did not understand the \"native\" datetime format (2017-08-26 22:00:00-05:00) that was exported to csv, so I had to create a new \"pretty\" date column (2017-08-26 22:00:00). I had lots of challenges because I could not format both a time and date \n.stftime()\n from the \n.date()\n method, nor the \n.time()\n method. It would only understand it's own type. So, I created strings of the date and the time and then put them together. Since I was exporting, it didn't matter that it was text and not a true datetime object.:\n\n\n  # create a tableau-friendly date\n  flow_central = flow_central.compute([\n          ('Measurement time', agate.Formula(agate.Text(),\n                                       lambda r: str(r['CentralTime'].date())\\\n                                       + \n \n \\\n                                       + str(r['CentralTime'].time()))) \n      ])", 
            "title": "Compute"
        }, 
        {
            "location": "/compute/#new-columns-using-compute", 
            "text": "Creating new columns based on something in the data.  [I need to explain a basic .compute() here.]  Items to add:   new date?", 
            "title": "New columns: Using .compute()"
        }, 
        {
            "location": "/compute/#zfill-fix-zero-padded-ids", 
            "text": "In school data from TEA, the  Campus_ID  column is supposed to be a 9-digit column, and any school that is less than that should have zeros at the beginning. Sometimes .csv files saved from Excel loose this zero padding, but we can replace it:    zfilled = nodistrict.compute([\n      ('New_Campus_ID', agate.Formula(agate.Text(), lambda r: r['CAMPUS'].zfill(9))),\n      ('New_District_ID', agate.Formula(agate.Text(), lambda r: r['DISTRICT'].zfill(6)))\n  ])  For the first one, we created  New_Campus_ID  by going through the  Campus  column and using the Python method  .zfill()  to set the number of characters to pad with zeros. For  ['Campus']  we used  9  and for  ['District']  we use  6 .", 
            "title": "zfill: Fix zero-padded IDs"
        }, 
        {
            "location": "/compute/#mapped-translation", 
            "text": "We want to create a new field that inserts the longer explanation based on the a one-letter designation in the data.  First we have the map:    rating_map = {\n      'I': 'Improvement required',\n      'M': 'Met standard',\n      'A': 'Met alternative standard',\n      'X': 'Not rated',\n      'Z': 'Not rated',\n      '': 'Not rated'\n  }  Then we need a function that we will run the compute through to get the proper rating match:    def map_rating(rating):\n      rating = rating.strip()\n      return rating_map[rating]  Then we create the new table. We are looking at the field  C_RATING  for the single-letter values. (This was originally used as a method on an export command, so it should be tested in this configuration):    rated = unrated.compute([\n      ('mapped_rating',\n       agate.Formula(agate.Text(),\n       lambda r: map_rating(r['C_RATING']))\n      )\n  ])  In then end, we have a new column called  mapped_rating  that includes values like \"Met Standard\".", 
            "title": "Mapped translation"
        }, 
        {
            "location": "/compute/#converting-timezones", 
            "text": "I had data that came in UTC time, but I wanted to display it in Central Time. You do first need to make sure your datatime is not naive and has a timezone, perhaps when :ref: you import it  importtime .  In this case, I had a field  dateTime  that was in UTC, that I need to convert to Central Time. This requires  the  pytz library  http://pytz.sourceforge.net/index.html?highlight=list%20timezones# _, as well:    ## setting central time\n  central = pytz.timezone('US/Central')\n\n  ## formula to do the converstion from the 'dateTime' field\n  time_shifter = agate.Formula(agate.DateTime(), lambda r: r['dateTime'].astimezone(central))\n\n  ## create column and call the formula above\n  flow_central = flow_data.compute([\n          ('CentralTime', time_shifter),\n      ])  This gives me the new column  Central Time .", 
            "title": "Converting timezones"
        }, 
        {
            "location": "/compute/#rewrite-datetime-in-another-format", 
            "text": "I had a case where my Tableau did not understand the \"native\" datetime format (2017-08-26 22:00:00-05:00) that was exported to csv, so I had to create a new \"pretty\" date column (2017-08-26 22:00:00). I had lots of challenges because I could not format both a time and date  .stftime()  from the  .date()  method, nor the  .time()  method. It would only understand it's own type. So, I created strings of the date and the time and then put them together. Since I was exporting, it didn't matter that it was text and not a true datetime object.:    # create a tableau-friendly date\n  flow_central = flow_central.compute([\n          ('Measurement time', agate.Formula(agate.Text(),\n                                       lambda r: str(r['CentralTime'].date())\\\n                                       +     \\\n                                       + str(r['CentralTime'].time()))) \n      ])", 
            "title": "Rewrite date/time in another format"
        }
    ]
}