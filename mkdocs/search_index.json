{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Agate Examples\n\n\n[This is a young work in progress.]\n\n\nThis site is a collection of my personal examples using \nagate\n Python library. While the \nCookbook\n and \nTutorial\n are a good start to learning and using agate, these pages are a compendium of examples and variations that I have used and would like to remember. Perhaps they are useful to others, as well.\n\n\nI use \nagate\n in my work at the Statesman to build transparent, repeatable data pipelines and analysis in the course of reporting, usually in conjuction with \nJupyter Notebooks\n.\n\n\nUse of these examples implies some knowledge of agate, jupyter notebooks and python in general.\n\n\nSome examples of how we have used agate at the Statesman:\n\n\n\n\nMixed Beverage Receipts\n to see which establishment sold the most alcohol in a given month.\n\n\nOil spill data\n as it related to the Magellan Midstream Partners spill of July 13 in Bastrop County.\n\n\nTexas public school accountability ratings\n to prepare data for an interactive.\n\n\n\n\nAbout the author\n\n\nMy name is \nChristian McDonald\n, and I am the data editor at the Austin American-Stateman and an adjunct professor that the UT-Austin School of Journalism, where I teach a data journalism class.\n\n\nA start\n\n\n\n\nImports\n\n\nFilters\n\n\nNew columns", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-agate-examples", 
            "text": "[This is a young work in progress.]  This site is a collection of my personal examples using  agate  Python library. While the  Cookbook  and  Tutorial  are a good start to learning and using agate, these pages are a compendium of examples and variations that I have used and would like to remember. Perhaps they are useful to others, as well.  I use  agate  in my work at the Statesman to build transparent, repeatable data pipelines and analysis in the course of reporting, usually in conjuction with  Jupyter Notebooks .  Use of these examples implies some knowledge of agate, jupyter notebooks and python in general.  Some examples of how we have used agate at the Statesman:   Mixed Beverage Receipts  to see which establishment sold the most alcohol in a given month.  Oil spill data  as it related to the Magellan Midstream Partners spill of July 13 in Bastrop County.  Texas public school accountability ratings  to prepare data for an interactive.", 
            "title": "Welcome to Agate Examples"
        }, 
        {
            "location": "/#about-the-author", 
            "text": "My name is  Christian McDonald , and I am the data editor at the Austin American-Stateman and an adjunct professor that the UT-Austin School of Journalism, where I teach a data journalism class.", 
            "title": "About the author"
        }, 
        {
            "location": "/#a-start", 
            "text": "Imports  Filters  New columns", 
            "title": "A start"
        }, 
        {
            "location": "/imports/", 
            "text": "Imports \n Exports\n\n\n\n\nTo do:\n\n\n\n\nadd xlsx hints, if any \n\n\nadd a loop of imports for a series of files that are similar except for an id or filename. So I can update the list of IDs or names and they would all import, then I can \n.merge()\n them together. I can use our lcra-hydromet example, but perhaps with a different dataset of csvs (mixbev or hotels) for ease of understanding.\n\n\n\n\n\n\nSet column type\n\n\nMore often than not I have to set some of the columns to a specific data type when importing. An example might be to ensure a FIPS code or ZIP is considered Text instead of a Number:\n\n\nspecified_types = {\n    'column_name_one': agate.Text(),\n    'column_name_two': agate.Number()\n}\n\ntable = agate.Table.from_csv('filename.csv', column_types=specified_types)\n\n\n\n\nSet date format\n\n\nspecified_types = {\n    'date_col': agate.Date('%Y%m%d'),\n}\n\n\n\n\nAnd set the value inside \n.Date()\n based on \nstrftime formats\n.\n\n\nAdd timezone to a date\n\n\nI had a case where my original data was in UTC time, but I needed to convert it to Central time. According to \nagate docs on dates\n, it imports dates naive of timezone, so you have to set it to a specific timezone before you can convert it to a different one:\n\n\nimport pytz\n\n## sets date coming in at UTC so we can convert it later\nspecified_type = {\n    'dateTime': agate.DateTime(timezone=pytz.utc)\n}\n\n# import all the file.\nflow_08154700 = agate.Table.from_json(\n    '../downloads/flow-08154700.json',\n    column_types=specified_type, key='records'\n)\n\n\n\n\nNote you need to import the \npytz library\n. You can get a \nlist timezones here\n (or make your own).\n\n\nIn this case, the column name for the date in my data was \"dateTime\", not to be confused with the \n.DateTime()\n data type used in the definition.\n\n\nSee the \ncompute page\n for the conversion.\n\n\nImport from json\n\n\nThe example above also shows importing from a \n.json\n file::\n\n\nflow_08154700 = agate.Table.from_json(\n    '../downloads/flow-08154700.json',\n    column_types=specified_type, key='records'\n)\n\n\n\n\nNote you need a key, which is the part of the json file that defines the list of records (notably called \"records\" in this instance). \n\n\nMy data looked like this:\n\n\n{\n      \"siteId\":\"08154700\",\n      \"siteNumber\":\"08154700\",\n      \"siteName\":\"Bull Ck at Loop 360 nr Austin, TX\",\n      \"bankFullStage\":5,\n      \"value1Type\":\"Stage\",\n      \"value2Type\":\"Flow\",\n      \"records\":[\n          {\n              \"dateTime\":\"2017-08-27T03:00:00Z\",\n              \"value1\":4.46,\n              \"value1Qualifier\":\"P\",\n              \"value2\":414.00,\n              \"value2Qualifier\":\"P\"\n          },\n          {\n              \"dateTime\":\"2017-08-27T02:55:00Z\",\n              \"value1\":4.38,\n              \"value1Qualifier\":\"P\",\n              \"value2\":383.00,\n              \"value2Qualifier\":\"P\"\n      }\n  ]}\n\n\nI only needed the \"records\" nodes for my table, so the value I needed for the key was \nrecords\n. I'm not even sure how I would get the other information at the top.", 
            "title": "Imports"
        }, 
        {
            "location": "/imports/#imports-exports", 
            "text": "To do:   add xlsx hints, if any   add a loop of imports for a series of files that are similar except for an id or filename. So I can update the list of IDs or names and they would all import, then I can  .merge()  them together. I can use our lcra-hydromet example, but perhaps with a different dataset of csvs (mixbev or hotels) for ease of understanding.", 
            "title": "Imports &amp; Exports"
        }, 
        {
            "location": "/imports/#set-column-type", 
            "text": "More often than not I have to set some of the columns to a specific data type when importing. An example might be to ensure a FIPS code or ZIP is considered Text instead of a Number:  specified_types = {\n    'column_name_one': agate.Text(),\n    'column_name_two': agate.Number()\n}\n\ntable = agate.Table.from_csv('filename.csv', column_types=specified_types)", 
            "title": "Set column type"
        }, 
        {
            "location": "/imports/#set-date-format", 
            "text": "specified_types = {\n    'date_col': agate.Date('%Y%m%d'),\n}  And set the value inside  .Date()  based on  strftime formats .", 
            "title": "Set date format"
        }, 
        {
            "location": "/imports/#add-timezone-to-a-date", 
            "text": "I had a case where my original data was in UTC time, but I needed to convert it to Central time. According to  agate docs on dates , it imports dates naive of timezone, so you have to set it to a specific timezone before you can convert it to a different one:  import pytz\n\n## sets date coming in at UTC so we can convert it later\nspecified_type = {\n    'dateTime': agate.DateTime(timezone=pytz.utc)\n}\n\n# import all the file.\nflow_08154700 = agate.Table.from_json(\n    '../downloads/flow-08154700.json',\n    column_types=specified_type, key='records'\n)  Note you need to import the  pytz library . You can get a  list timezones here  (or make your own).  In this case, the column name for the date in my data was \"dateTime\", not to be confused with the  .DateTime()  data type used in the definition.  See the  compute page  for the conversion.", 
            "title": "Add timezone to a date"
        }, 
        {
            "location": "/imports/#import-from-json", 
            "text": "The example above also shows importing from a  .json  file::  flow_08154700 = agate.Table.from_json(\n    '../downloads/flow-08154700.json',\n    column_types=specified_type, key='records'\n)  Note you need a key, which is the part of the json file that defines the list of records (notably called \"records\" in this instance).   My data looked like this:  {\n      \"siteId\":\"08154700\",\n      \"siteNumber\":\"08154700\",\n      \"siteName\":\"Bull Ck at Loop 360 nr Austin, TX\",\n      \"bankFullStage\":5,\n      \"value1Type\":\"Stage\",\n      \"value2Type\":\"Flow\",\n      \"records\":[\n          {\n              \"dateTime\":\"2017-08-27T03:00:00Z\",\n              \"value1\":4.46,\n              \"value1Qualifier\":\"P\",\n              \"value2\":414.00,\n              \"value2Qualifier\":\"P\"\n          },\n          {\n              \"dateTime\":\"2017-08-27T02:55:00Z\",\n              \"value1\":4.38,\n              \"value1Qualifier\":\"P\",\n              \"value2\":383.00,\n              \"value2Qualifier\":\"P\"\n      }\n  ]}  I only needed the \"records\" nodes for my table, so the value I needed for the key was  records . I'm not even sure how I would get the other information at the top.", 
            "title": "Import from json"
        }, 
        {
            "location": "/filters/", 
            "text": "Filters: Using .where()\n\n\n\n\nTo add here:\n\n\n\n\nwhere between dates\n\n\ngrep\n\n\n\n\n\n\nExamples for the \n.where()\n method.\n\n\nMost of these examples are where I am creating a new table with a subselection of rows from the base table:\n\n\n  newtable = oldtable.where(lambda row: row['Column_Name'] == 'something')\n\n\n\n\nLamda is a function that happens just once in place. In the example above, I'm looking through each row, specifically at the column \nColumn_Name\n for the text 'something'. If it matches, I keep it and put it in the newtable. If not, it is discarded.\n\n\nWhere boolean field is True\n\n\nChecking if a boolean field is true or false:\n\n\n  charters = campus.where(lambda row: row['CFLCHART'] == 1)\n\n\n\n\n\n\nNote to self: Test if this will work as \n== True\n.\n\n\n\n\nWhere boolean field is not True\n\n\nWhen you want fields that don't match your test.:\n\n\n  traditional = campus.where(lambda row: row['CFLCHART'] != 1)\n\n\n\n\nIn this case, we are testing 'CFLCHART' to make sure it does NOT equal 1.\n\n\n\n\nNote to self: Test if this would catch nulls.)\n\n\n\n\nBy position in string\n\n\nIf you want to look at a specific position in a string and then check it:\n\n\n  charters = campus.where(lambda row: row['Campus_ID'][3] == '8')\n\n\n\n\nThe example is searching the \nCampus_ID\n field looking at the fourth character, checking to see if it is the text \n'8'\n.\n\n\nCheck for blank cells\n\n\nIn this example, I wanted to exclude rows that were null or blank in the \nCampus\n column:\n\n\n  nodistrict = raw.where(lambda row: row['CAMPUS'] is not None)\n\n\n\n\nFilter by date\n\n\nIn this case, I wanted only rows where the \nCentralTime\n date was on or after August 25. The trick here was importing \ndatetime\n, and setting that reference as below. It wasn't intuitive for me from the docs.\n\n\n  import datetime\n\n  # filter for Aug. 25th or later\n  flow_harvey = flow_central.where(\n      lambda row: datetime.date(2017, 8, 25) \n= row['CentralTime'].date()\n      )\n\n\n\n\nFilter by list of items\n\n\nI this case, I want rows that match items out of a list, i.e., I rows only from Austin MSA counties.\n\n\n# list of counties to keep\ncounty_list = [\n    \nBastrop\n,\n    \nCaldwell\n,\n    \nHays\n,\n    \nTravis\n,\n    \nWilliamson\n\n]\n\n# function to test a column value against a list\ndef list_filter(row, list_to_check):\n    if row is None:\n        return True\n    myList = row.split(',')\n    for item in myList:\n        if item not in list_to_check:\n            return False\n    return True\n\n# Pass in the column to check, and then the list. Only true rows pass.\naustin_msa = raw.where(lambda row: list_filter(row['County'], county_list))\n\n\n\n\n\nDistinct values of a column\n\n\nIf you want to see the distinct values of a column, you can call the \n.distinct_values()\n method on that column and see the results in a tuple.\n\n\nrated.columns['C_RATING'].values_distinct()\n\n\n\n\nrated\n is the name of the table in this case, and \nC_RATING\n is the column. The result looks like this:\n\n\n('A', 'I', 'X', 'Z', 'T', 'M')", 
            "title": "Filters"
        }, 
        {
            "location": "/filters/#filters-using-where", 
            "text": "To add here:   where between dates  grep    Examples for the  .where()  method.  Most of these examples are where I am creating a new table with a subselection of rows from the base table:    newtable = oldtable.where(lambda row: row['Column_Name'] == 'something')  Lamda is a function that happens just once in place. In the example above, I'm looking through each row, specifically at the column  Column_Name  for the text 'something'. If it matches, I keep it and put it in the newtable. If not, it is discarded.", 
            "title": "Filters: Using .where()"
        }, 
        {
            "location": "/filters/#where-boolean-field-is-true", 
            "text": "Checking if a boolean field is true or false:    charters = campus.where(lambda row: row['CFLCHART'] == 1)   Note to self: Test if this will work as  == True .", 
            "title": "Where boolean field is True"
        }, 
        {
            "location": "/filters/#where-boolean-field-is-not-true", 
            "text": "When you want fields that don't match your test.:    traditional = campus.where(lambda row: row['CFLCHART'] != 1)  In this case, we are testing 'CFLCHART' to make sure it does NOT equal 1.   Note to self: Test if this would catch nulls.)", 
            "title": "Where boolean field is not True"
        }, 
        {
            "location": "/filters/#by-position-in-string", 
            "text": "If you want to look at a specific position in a string and then check it:    charters = campus.where(lambda row: row['Campus_ID'][3] == '8')  The example is searching the  Campus_ID  field looking at the fourth character, checking to see if it is the text  '8' .", 
            "title": "By position in string"
        }, 
        {
            "location": "/filters/#check-for-blank-cells", 
            "text": "In this example, I wanted to exclude rows that were null or blank in the  Campus  column:    nodistrict = raw.where(lambda row: row['CAMPUS'] is not None)", 
            "title": "Check for blank cells"
        }, 
        {
            "location": "/filters/#filter-by-date", 
            "text": "In this case, I wanted only rows where the  CentralTime  date was on or after August 25. The trick here was importing  datetime , and setting that reference as below. It wasn't intuitive for me from the docs.    import datetime\n\n  # filter for Aug. 25th or later\n  flow_harvey = flow_central.where(\n      lambda row: datetime.date(2017, 8, 25)  = row['CentralTime'].date()\n      )", 
            "title": "Filter by date"
        }, 
        {
            "location": "/filters/#filter-by-list-of-items", 
            "text": "I this case, I want rows that match items out of a list, i.e., I rows only from Austin MSA counties.  # list of counties to keep\ncounty_list = [\n     Bastrop ,\n     Caldwell ,\n     Hays ,\n     Travis ,\n     Williamson \n]\n\n# function to test a column value against a list\ndef list_filter(row, list_to_check):\n    if row is None:\n        return True\n    myList = row.split(',')\n    for item in myList:\n        if item not in list_to_check:\n            return False\n    return True\n\n# Pass in the column to check, and then the list. Only true rows pass.\naustin_msa = raw.where(lambda row: list_filter(row['County'], county_list))", 
            "title": "Filter by list of items"
        }, 
        {
            "location": "/filters/#distinct-values-of-a-column", 
            "text": "If you want to see the distinct values of a column, you can call the  .distinct_values()  method on that column and see the results in a tuple.  rated.columns['C_RATING'].values_distinct()  rated  is the name of the table in this case, and  C_RATING  is the column. The result looks like this:  ('A', 'I', 'X', 'Z', 'T', 'M')", 
            "title": "Distinct values of a column"
        }, 
        {
            "location": "/compute/", 
            "text": "New columns: Using .compute()\n\n\nCreating new columns based on something in the data.\n\n\nThis extends the \n.compute() examples\n\n\nzfill: Fix zero-padded IDs\n\n\nIn school data from TEA, the \nCampus_ID\n column is supposed to be a 9-digit column, and any school that is less than that should have zeros at the beginning. Sometimes .csv files saved from Excel loose this zero padding, but we can replace it:\n\n\n  zfilled = nodistrict.compute([\n      ('New_Campus_ID', agate.Formula(agate.Text(), lambda r: r['CAMPUS'].zfill(9))),\n      ('New_District_ID', agate.Formula(agate.Text(), lambda r: r['DISTRICT'].zfill(6)))\n  ])\n\n\n\n\nFor the first one, we created \nNew_Campus_ID\n by going through the \nCampus\n column and using the Python method \n.zfill()\n to set pad any \nCampus\n that wasn't 9 characters. For  \n['District']\n we used \"6\".\n\n\nMapped translation\n\n\nWe wanted to create a new field that inserts the longer explanation based on the a one-letter designation in the data.\n\n\nFirst we have the map:\n\n\n  rating_map = {\n      'I': 'Improvement required',\n      'M': 'Met standard',\n      'A': 'Met alternative standard',\n      'X': 'Not rated',\n      'Z': 'Not rated',\n      '': 'Not rated'\n  }\n\n\n\n\nThen we need a function that we will run the compute through to get the proper rating match:\n\n\n  def map_rating(rating):\n      rating = rating.strip()\n      return rating_map[rating]\n\n\n\n\nThen we create the new table. We are looking at the field \nC_RATING\n for the single-letter values. (This was originally used as a method on an export command, so it should be tested in this configuration):\n\n\n  rated = unrated.compute([\n      ('mapped_rating',\n       agate.Formula(agate.Text(),\n       lambda r: map_rating(r['C_RATING']))\n      )\n  ])\n\n\n\n\nIn then end, we have a new column called \nmapped_rating\n that includes values like \"Met Standard\".\n\n\nConverting timezones\n\n\nI had data that came in UTC time, but I wanted to display it in Central Time. You first need to make sure your datatime is not naive and has a timezone, perhaps when \nyou import it\n.\n\n\nIn this case, I had a field \ndateTime\n that was in UTC, that I need to convert to Central Time. This requires  the \npytz library\n, as well:\n\n\n  ## Import pytz if you don't already have it\n  import pytz\n\n  ## setting central time\n  central = pytz.timezone('US/Central')\n\n  ## formula to do the conversion from the 'dateTime' field\n  time_shifter = agate.Formula(\n    agate.DateTime(),\n    lambda r: r['dateTime'].astimezone(central)\n    )\n\n  ## create column and call the formula above\n  flow_central = flow_data.compute([\n          ('Central Time', time_shifter),\n      ])\n\n\n\n\nThis gives me the new column \nCentral Time\n.\n\n\nRewrite date/time in another format\n\n\nI had a case where Tableau did not understand the \"native\" datetime format (2017-08-26 22:00:00-05:00) that was exported to csv, so I had to create a new \"pretty\" date column (2017-08-26 22:00:00). I had lots of challenges because I could not format both a time and date \n.stftime()\n from the \n.date()\n method, nor the \n.time()\n method. It would only understand it's own type. So, I created strings of the date and the time and then put them together. Since I was exporting, it didn't matter that it was text and not a true datetime object.:\n\n\n  # create a tableau-friendly date\n  flow_central = flow_central.compute([\n          ('Measurement time', agate.Formula(agate.Text(),\n                                       lambda r: str(r['CentralTime'].date())\\\n                                       + \n \n \\\n                                       + str(r['CentralTime'].time()))) \n      ])", 
            "title": "Compute"
        }, 
        {
            "location": "/compute/#new-columns-using-compute", 
            "text": "Creating new columns based on something in the data.  This extends the  .compute() examples", 
            "title": "New columns: Using .compute()"
        }, 
        {
            "location": "/compute/#zfill-fix-zero-padded-ids", 
            "text": "In school data from TEA, the  Campus_ID  column is supposed to be a 9-digit column, and any school that is less than that should have zeros at the beginning. Sometimes .csv files saved from Excel loose this zero padding, but we can replace it:    zfilled = nodistrict.compute([\n      ('New_Campus_ID', agate.Formula(agate.Text(), lambda r: r['CAMPUS'].zfill(9))),\n      ('New_District_ID', agate.Formula(agate.Text(), lambda r: r['DISTRICT'].zfill(6)))\n  ])  For the first one, we created  New_Campus_ID  by going through the  Campus  column and using the Python method  .zfill()  to set pad any  Campus  that wasn't 9 characters. For   ['District']  we used \"6\".", 
            "title": "zfill: Fix zero-padded IDs"
        }, 
        {
            "location": "/compute/#mapped-translation", 
            "text": "We wanted to create a new field that inserts the longer explanation based on the a one-letter designation in the data.  First we have the map:    rating_map = {\n      'I': 'Improvement required',\n      'M': 'Met standard',\n      'A': 'Met alternative standard',\n      'X': 'Not rated',\n      'Z': 'Not rated',\n      '': 'Not rated'\n  }  Then we need a function that we will run the compute through to get the proper rating match:    def map_rating(rating):\n      rating = rating.strip()\n      return rating_map[rating]  Then we create the new table. We are looking at the field  C_RATING  for the single-letter values. (This was originally used as a method on an export command, so it should be tested in this configuration):    rated = unrated.compute([\n      ('mapped_rating',\n       agate.Formula(agate.Text(),\n       lambda r: map_rating(r['C_RATING']))\n      )\n  ])  In then end, we have a new column called  mapped_rating  that includes values like \"Met Standard\".", 
            "title": "Mapped translation"
        }, 
        {
            "location": "/compute/#converting-timezones", 
            "text": "I had data that came in UTC time, but I wanted to display it in Central Time. You first need to make sure your datatime is not naive and has a timezone, perhaps when  you import it .  In this case, I had a field  dateTime  that was in UTC, that I need to convert to Central Time. This requires  the  pytz library , as well:    ## Import pytz if you don't already have it\n  import pytz\n\n  ## setting central time\n  central = pytz.timezone('US/Central')\n\n  ## formula to do the conversion from the 'dateTime' field\n  time_shifter = agate.Formula(\n    agate.DateTime(),\n    lambda r: r['dateTime'].astimezone(central)\n    )\n\n  ## create column and call the formula above\n  flow_central = flow_data.compute([\n          ('Central Time', time_shifter),\n      ])  This gives me the new column  Central Time .", 
            "title": "Converting timezones"
        }, 
        {
            "location": "/compute/#rewrite-datetime-in-another-format", 
            "text": "I had a case where Tableau did not understand the \"native\" datetime format (2017-08-26 22:00:00-05:00) that was exported to csv, so I had to create a new \"pretty\" date column (2017-08-26 22:00:00). I had lots of challenges because I could not format both a time and date  .stftime()  from the  .date()  method, nor the  .time()  method. It would only understand it's own type. So, I created strings of the date and the time and then put them together. Since I was exporting, it didn't matter that it was text and not a true datetime object.:    # create a tableau-friendly date\n  flow_central = flow_central.compute([\n          ('Measurement time', agate.Formula(agate.Text(),\n                                       lambda r: str(r['CentralTime'].date())\\\n                                       +     \\\n                                       + str(r['CentralTime'].time()))) \n      ])", 
            "title": "Rewrite date/time in another format"
        }, 
        {
            "location": "/sort/", 
            "text": "Sorting\n\n\nSimple sort\n\n\nnew_table = table.order_by('birth_date', reverse=True)\n\n\n\n\nSort by multiple columns\n\n\nThis is in the \nagate docs\n, but ...\n\n\nsorted_table = data.order_by(lambda row: (row['county'], row['city']))\n\n\n\n\nThe new \nsorted_table\n will be ordered by \ncounty\n, then \ncity\n.", 
            "title": "Sorting"
        }, 
        {
            "location": "/sort/#sorting", 
            "text": "", 
            "title": "Sorting"
        }, 
        {
            "location": "/sort/#simple-sort", 
            "text": "new_table = table.order_by('birth_date', reverse=True)", 
            "title": "Simple sort"
        }, 
        {
            "location": "/sort/#sort-by-multiple-columns", 
            "text": "This is in the  agate docs , but ...  sorted_table = data.order_by(lambda row: (row['county'], row['city']))  The new  sorted_table  will be ordered by  county , then  city .", 
            "title": "Sort by multiple columns"
        }
    ]
}